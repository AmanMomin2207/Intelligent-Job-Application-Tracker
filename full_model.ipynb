{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b6bb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üì¶ Block 1: Setup & Imports\n",
    "# =============================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import joblib\n",
    "import fitz  # PyMuPDF\n",
    "import unicodedata\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from rapidfuzz import process\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a835722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text extraction and cleaning functions are ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üìÑ Block 2: Text Extraction + Cleaning\n",
    "# =============================\n",
    "\n",
    "# -------- PDF Extraction --------\n",
    "def text_from_pdf(path: str) -> str:\n",
    "    doc = fitz.open(path)\n",
    "    text_blocks = [page.get_text(\"text\") for page in doc]\n",
    "    return \"\\n\".join(text_blocks)\n",
    "\n",
    "# -------- DOCX Extraction --------\n",
    "def text_from_docx(path: str) -> str:\n",
    "    doc = Document(path)\n",
    "    paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "# -------- Image (OCR) Extraction --------\n",
    "def text_from_image(path: str) -> str:\n",
    "    img = Image.open(path)\n",
    "    return pytesseract.image_to_string(img)\n",
    "\n",
    "# -------- TXT Extraction --------\n",
    "def text_from_txt(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# -------- Unified Extractor --------\n",
    "def extract_text(path: str) -> str:\n",
    "    ext = Path(path).suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        return text_from_pdf(path)\n",
    "    elif ext == \".docx\":\n",
    "        return text_from_docx(path)\n",
    "    elif ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        return text_from_image(path)\n",
    "    elif ext == \".txt\":\n",
    "        return text_from_txt(path)\n",
    "    else:\n",
    "        raise ValueError(f\"‚ùå Unsupported file type: {ext}\")\n",
    "\n",
    "\n",
    "# -------- Cleaning Function --------\n",
    "def clean_resume_text(text):\n",
    "    \"\"\"\n",
    "    Cleans extracted resume text to make it model-ready.\n",
    "    Fixes spacing, normalization, and unwanted artifacts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Fix cases like 'A N I S' ‚Üí 'ANIS'\n",
    "    text = re.sub(r'(?<=\\b[A-Z])(?:\\s(?=[A-Z]\\b))+', '', text)\n",
    "\n",
    "    # Fix random double spaces ‚Üí single space\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # Fix broken lines like \"P R O J E C T S\" ‚Üí \"PROJECTS\"\n",
    "    text = re.sub(r'(\\b[A-Z]\\s){2,}[A-Z]\\b', lambda m: m.group(0).replace(' ', ''), text)\n",
    "\n",
    "    # Preserve proper newlines between sections\n",
    "    text = re.sub(r'(\\n\\s*){2,}', '\\n\\n', text)\n",
    "\n",
    "    # Remove extra symbols or underscores\n",
    "    text = re.sub(r'[_‚Ä¢¬∑‚óè‚óÜ‚ñ∂-]+', '', text)\n",
    "\n",
    "    # Strip trailing/leading whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ Text extraction and cleaning functions are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0277d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMAN SHIKALGAR Kolhapur, Maharashtra +918793979556 armanshikalgar01@gmail.com https://www.linkedin.com/in/arman88 GitHub Portfolio Profile Summary  Final year B. Tech student specializing in Artificial Intelligence and Data Science.  Open to internships or fulltime roles in data analytics, python role and frontend developer. Experience Python Intern ‚Äì CodSoft Virtual Internship | June 2024 ‚Äì June 2024  Completed projectbased internship focused on Python programming fundamentals and practical im\n"
     ]
    }
   ],
   "source": [
    "resume_text = extract_text(\"temp_Arman Resume 2025.pdf\")\n",
    "clean_text = clean_resume_text(resume_text)\n",
    "print(clean_text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ffd04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_broken_spacing_v2(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Final version ‚Äî repairs PDFs with broken or glued text:\n",
    "    - Fixes spaced letters (\"M A C H I N E\" -> \"MACHINE\")\n",
    "    - Preserves valid spaces (\"Machine Learning\")\n",
    "    - Restores emails and city names\n",
    "    \"\"\"\n",
    "    # Step 1: Remove extra spaces but keep structure\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # Step 2: Fix all-caps split words: \"D A T A\" ‚Üí \"DATA\"\n",
    "    text = re.sub(r'(?<=\\b)(?:[A-Z]\\s){2,}[A-Z]\\b', lambda m: m.group(0).replace(\" \", \"\"), text)\n",
    "\n",
    "    # Step 3: Fix mixed-case split words: \"M a c h i n e\" ‚Üí \"Machine\"\n",
    "    text = re.sub(r'(?<=\\b)(?:[A-Za-z]\\s){2,}[A-Za-z]\\b', lambda m: m.group(0).replace(\" \", \"\"), text)\n",
    "\n",
    "    # Step 4: Fix emails like \"a n i s @ g m a i l . c o m\"\n",
    "    text = re.sub(r'((?:[a-zA-Z0-9]\\s?)+@\\s?(?:[a-zA-Z]\\s?)+\\.\\s?(?:[a-zA-Z]{2,}))',\n",
    "                  lambda m: m.group(0).replace(\" \", \"\"), text)\n",
    "\n",
    "    # Step 5: Fix phone numbers with spaces\n",
    "    text = re.sub(r'(\\+?\\d[\\d\\s\\-]{8,}\\d)', lambda m: re.sub(r'\\s+', '', m.group(0)), text)\n",
    "\n",
    "    # Step 6: Fix cities/states like \"M a h a r a s t r a\" ‚Üí \"Maharastra\"\n",
    "    text = re.sub(r'(?<=\\b)(?:[A-Za-z]\\s){2,}[A-Za-z]\\b', lambda m: m.group(0).replace(\" \", \"\"), text)\n",
    "\n",
    "    # Step 7: Normalize punctuation and remove artifacts\n",
    "    text = re.sub(r'\\s+([,.;:!?])', r'\\1', text)\n",
    "    text = re.sub(r'[_‚Ä¢¬∑‚óè‚óÜ‚ñ∂-]+', '', text)\n",
    "\n",
    "    # Step 8: Final cleanup\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9660eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMAN SHIKALGAR Kolhapur, Maharashtra +918793979556armanshikalgar01@gmail.com https://www.linkedin.com/in/arman88 GitHub Portfolio Profile Summary Final year B. Tech student specializing in Artificial Intelligence and Data Science. Open to internships or fulltime roles in data analytics, python role and frontend developer. Experience Python Intern ‚Äì CodSoft Virtual Internship | June 2024 ‚Äì June 2024 Completed projectbased internship focused on Python programming fundamentals and practical implementation. Enhanced debugging, logical thinking, and clean code practices during handson tasks. Web Development Intern ‚Äì CodeClause Virtual Internship | Apr 2024 ‚Äì Apr 2024 Built responsive and interactive web applications using HTML, CSS, and JavaScript. Gained handson experience in layout design, component structuring, and version control with GitHub. Projects PDF Chat Bot | Python, Streamlit, Google Gemini API, Huggingface | Repository | Live Preview Streamlit based chatbot that interact with uploaded PDFs file using Langchain. Part of the Thirdyear academic mini project. Movies Manager | Python, PyMongo, Streamlit, MongoDB Atlas Cloud | Repository | Live Preview Developed a Streamlitbased web application to manage movie records with CRUD functionality. Implemented MongoDB Atlas as a cloud database and used PyMongo for database interactions. Technical Skills Languages: Python, JavaScript, HTML, CSS, SQL Databases & Tools: MongoDB, MySQL, Excel, Git & GitHub Libraries & Frameworks: Django, Pandas, NumPy, Matplotlib, Seaborn, Streamlit Other Skills: Figma (basic), Power BI, Prompt Engineering, Hugging Face Deployment Education Dr. J. J. Magdum College of Engineering, Jaysingpur | 2023‚Äì2026 (Currently in Final Year) B. Tech in AI & Data Science ‚Äì CGPA:7.00 Institute of Civil and Rural Engineering, Gargoti | 20202023 | Diploma in Computer Science Engineering. Extracurricular / Certificates Google Data Analytics | Coursera | View Credentials Delivered a technical seminar on Git and GitHub to peers in college. Runnerup in a Prompt Engineering competition at Annasaheb Dange College of Engineering.\n"
     ]
    }
   ],
   "source": [
    "refined = fix_broken_spacing_v2(clean_text)\n",
    "print(refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b40f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracted Entities:\n",
      "{'name': 'Streamlit', 'phone': '+918793979556', 'email': '+918793979556armanshikalgar01@gmail.com', 'linkedin': 'https://www.linkedin.com/in/arman88', 'github': None, 'skills': ['CSS', 'Data Science', 'Django', 'Git', 'GitHub', 'HTML', 'JavaScript', 'Matplotlib', 'MongoDB', 'MySQL', 'NumPy', 'Pandas', 'Power BI', 'Python', 'SQL', 'Seaborn', 'Streamlit'], 'yoe': None, 'location': None, 'projects': [{'name': '. Movies Manager | Python, PyMongo, Streamlit, MongoDB Atlas Cloud | Repository | Live Preview Developed a Streamlitbased web application to manage movie records with CRUD functionality. Implemented MongoDB Atlas as a cloud database and used PyMongo for database interactions. Technical Skills Languages: Python, JavaScript, HTML, CSS, SQL Databases & Tools: MongoDB, MySQL, Excel, Git & GitHub Libraries & Frameworks: Django, Pandas, NumPy, Matplotlib, Seaborn, Streamlit Other Skills: Figma', 'tech_stack': ['CSS', 'Data Science', 'Django', 'Git', 'GitHub', 'HTML', 'JavaScript', 'Matplotlib', 'MongoDB', 'MySQL', 'NumPy', 'Pandas', 'Power BI', 'Python', 'SQL', 'Seaborn', 'Streamlit']}]}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# ===============================\n",
    "# üìå Load lightweight SpaCy model for PERSON detection\n",
    "# ===============================\n",
    "nlp_name = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ===============================\n",
    "# üìå Predefined Skills Database\n",
    "# ===============================\n",
    "SKILLS_DB = [\n",
    "    \"Java\",\"Spring Boot\",\"Python\",\"C++\",\"C#\",\"JavaScript\",\"TypeScript\",\"HTML\",\"CSS\",\"React\",\"Angular\",\"Vue.js\",\n",
    "    \"Node.js\",\"Express.js\",\"Django\",\"Flask\",\"REST\",\"GraphQL\",\"Microservices\",\"SQL\",\"MySQL\",\"PostgreSQL\",\"MongoDB\",\n",
    "    \"Oracle\",\"SQLite\",\"Redis\",\"Docker\",\"Kubernetes\",\"Jenkins\",\"Git\",\"GitHub\",\"Bitbucket\",\"GitLab\",\"AWS\",\"Azure\",\n",
    "    \"Google Cloud\",\"Linux\",\"CI/CD\",\"Agile\",\"Machine Learning\",\"Deep Learning\",\"TensorFlow\",\"PyTorch\",\"Pandas\",\n",
    "    \"NumPy\",\"Scikit-learn\",\"Hadoop\",\"Spark\",\"Tableau\",\"Power BI\",\"Streamlit\",\"Seaborn\",\"Matplotlib\",\"Data Science\"\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# üìå City/State Database\n",
    "# ===============================\n",
    "CITIES_AND_STATES = [\n",
    "    \"Mumbai\",\"Pune\",\"Delhi\",\"Bengaluru\",\"Chennai\",\"Hyderabad\",\"Kolhapur\",\"Mohol\",\"Solapur\",\"Nashik\",\"Nagpur\",\"Goa\",\n",
    "    \"Thane\",\"Aurangabad\",\"Ahmedabad\",\"Indore\",\"Jaipur\",\"Kolkata\",\"Surat\",\"Lucknow\",\"Ranchi\",\"Bhopal\",\"Patna\",\"Kanpur\",\n",
    "    \"Vadodara\",\"Noida\",\"Gurgaon\",\"Chandigarh\",\"Coimbatore\",\"Vizag\",\"Maharashtra\",\"Maharastra\",\"Karnataka\",\"Gujarat\",\n",
    "    \"Tamil Nadu\",\"Telangana\",\"West Bengal\",\"Rajasthan\",\"Madhya Pradesh\",\"Uttar Pradesh\",\"Punjab\",\"Haryana\",\"Odisha\",\n",
    "    \"Kerala\",\"Assam\",\"Ichalkaranji\",\"Jaysingpur\",\"Sangli\"\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# üìå Fuzzy Location Matcher\n",
    "# ===============================\n",
    "def get_best_location(text):\n",
    "    lines = [line for line in text.split(\"\\n\") if \",\" in line or \"|\" in line]\n",
    "    combined = \" \".join(lines) if lines else text\n",
    "    result = process.extractOne(combined, CITIES_AND_STATES, score_cutoff=70)\n",
    "    return result[0] if result else None\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# üìå Rule-Based Extraction (with improved name)\n",
    "# ===============================\n",
    "def extract_rule_based(text: str) -> dict:\n",
    "    extracted = {}\n",
    "\n",
    "     # ---- Improved Name Extraction (via SpaCy PERSON entity) ----\n",
    "    doc = nlp_name(text)\n",
    "    NAME_BLACKLIST = {\"ENGINEER\", \"DEVELOPER\", \"PROJECT\", \"MANAGER\", \"SUMMARY\"}\n",
    "    top_lines = text.strip().split(\"\\n\")[:5]\n",
    "\n",
    "    persons = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for p in persons:\n",
    "        if any(b in p.upper() for b in NAME_BLACKLIST):\n",
    "            continue\n",
    "        if 1 <= len(p.split()) <= 3:  # allow up to 3-word names\n",
    "            for line in top_lines:\n",
    "                if p in line:\n",
    "                    extracted[\"name\"] = p\n",
    "                    break\n",
    "        if extracted.get(\"name\"):\n",
    "            break\n",
    "    if not extracted.get(\"name\") and persons:\n",
    "        extracted[\"name\"] = persons[0]  # fallback\n",
    "\n",
    "    # ---- Phone ----\n",
    "    phone_pattern = r\"(\\+?\\d{1,3}[\\s-]?\\d{5}[\\s-]?\\d{5}|\\+?\\d{1,3}[\\s-]?\\d{10}|\\d{10})\"\n",
    "    phones = re.findall(phone_pattern, text)\n",
    "    extracted[\"phone\"] = phones[0] if phones else None\n",
    "\n",
    "    # ---- Email ----\n",
    "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    extracted[\"email\"] = emails[0] if emails else None\n",
    "\n",
    "    # ---- LinkedIn ----\n",
    "    linkedin_pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/[A-Za-z0-9\\/\\-\\_]+\"\n",
    "    linkedin = re.findall(linkedin_pattern, text)\n",
    "    extracted[\"linkedin\"] = linkedin[0] if linkedin else None\n",
    "\n",
    "    # ---- GitHub ----\n",
    "    github_pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?github\\.com\\/[A-Za-z0-9\\/\\-\\_]+\"\n",
    "    github = re.findall(github_pattern, text)\n",
    "    extracted[\"github\"] = github[0] if github else None\n",
    "\n",
    "    # ---- Skills ----\n",
    "    found_skills = [skill for skill in SKILLS_DB if re.search(rf\"\\b{re.escape(skill)}\\b\", text, re.IGNORECASE)]\n",
    "    extracted[\"skills\"] = sorted(set(found_skills))\n",
    "\n",
    "    # ---- Years of Experience ----\n",
    "    yoe_pattern = r\"(\\d+\\+?\\s*(?:years|yrs))\"\n",
    "    yoe_match = re.search(yoe_pattern, text, re.IGNORECASE)\n",
    "    extracted[\"yoe\"] = yoe_match.group(1) if yoe_match else None\n",
    "\n",
    "    # ---- Fuzzy Location ----\n",
    "    location = get_best_location(text)\n",
    "    extracted[\"location\"] = location\n",
    "\n",
    "    # ---- Project Extraction ----\n",
    "        # ---- Project Extraction (compressed) ----\n",
    "    projects = []\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.search(r'\\b(project|major project|minor project)\\b', line, re.IGNORECASE):\n",
    "            name = re.sub(r'(?i).*\\bproject[:\\-]?\\s*', '', line).split(\"(\")[0].strip(\" -:‚Ä¢\")\n",
    "            tech = [s for s in SKILLS_DB if re.search(rf\"\\b{s}\\b\", line, re.I)]\n",
    "\n",
    "            # Check nearby lines for tech stack\n",
    "            nearby = \" \".join(lines[i+1:i+3])\n",
    "            tech += [s for s in SKILLS_DB if re.search(rf\"\\b{s}\\b\", nearby, re.I)]\n",
    "            tech = sorted(set(tech))\n",
    "\n",
    "            projects.append({\"name\": name or None, \"tech_stack\": tech or None})\n",
    "\n",
    "    extracted[\"projects\"] = projects or None\n",
    "\n",
    "    return extracted\n",
    "\n",
    "\n",
    "print(\"üîç Extracted Entities:\")\n",
    "print(extract_rule_based(refined))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
